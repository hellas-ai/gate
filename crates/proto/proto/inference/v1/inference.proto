syntax = "proto3";
package gate.inference.v1;

import "common/v1/types.proto";
import "common/v1/errors.proto";

// Inference protocol for AI model interaction
// Provided by daemon nodes that support inference
service InferenceService {
  // Streaming inference for real-time token generation
  rpc StreamingInference(stream InferenceRequest) returns (stream InferenceResponse);
  
  // Traditional unary inference
  rpc Inference(InferenceRequest) returns (InferenceResponse);
  
  // Model management
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse);
  rpc LoadModel(LoadModelRequest) returns (stream LoadModelResponse);
  rpc UnloadModel(UnloadModelRequest) returns (UnloadModelResponse);
  
  // Model status and health
  rpc GetModelStatus(ModelStatusRequest) returns (ModelStatusResponse);
}

// Inference request
message InferenceRequest {
  string request_id = 1;        // Client-provided request ID for correlation
  string model_id = 2;          // Model to use for inference
  gate.common.v1.JsonValue input_data = 3;  // Model-specific input
  InferenceOptions options = 4;
  
  message InferenceOptions {
    bool streaming = 1;              // Enable streaming responses
    uint32 max_tokens = 2;           // Maximum tokens to generate
    float temperature = 3;           // Randomness (0.0 to 2.0)
    repeated string stop_sequences = 4; // Stop generation on these sequences
    map<string, string> custom_params = 5; // Model-specific parameters
  }
}

// Inference response (used for both streaming and unary)
message InferenceResponse {
  string request_id = 1;
  oneof response {
    InferenceChunk chunk = 2;
    InferenceComplete complete = 3;
    gate.common.v1.Error error = 4;
  }
  
  message InferenceChunk {
    gate.common.v1.JsonValue data = 1;  // Incremental data
    bool is_final = 2;                  // Last chunk in stream
    InferenceMetrics metrics = 3;       // Optional metrics
  }
  
  message InferenceComplete {
    gate.common.v1.JsonValue result = 1; // Final result
    InferenceMetrics metrics = 2;
  }
  
  message InferenceMetrics {
    uint32 tokens_processed = 1;
    uint32 tokens_generated = 2;
    uint64 latency_ms = 3;
    float tokens_per_second = 4;
    map<string, string> custom_metrics = 5;
  }
}

// Model listing
message ListModelsRequest {
  bool include_unavailable = 1;  // Include models that aren't loaded
}

message ListModelsResponse {
  repeated ModelInfo models = 1;
  
  message ModelInfo {
    string id = 1;
    string name = 2;
    string description = 3;
    repeated string capabilities = 4;  // "text", "code", "chat", "completion"
    ModelStatus status = 5;
    ModelSpecs specs = 6;
    
    enum ModelStatus {
      AVAILABLE = 0;    // Ready for inference
      LOADING = 1;      // Currently loading
      LOADED = 2;       // Loaded but not ready (warming up)
      ERROR = 3;        // Failed to load
      UNLOADED = 4;     // Not currently loaded
    }
    
    message ModelSpecs {
      uint64 parameters = 1;        // Number of parameters
      uint64 memory_required = 2;   // Memory required in bytes
      repeated string formats = 3;  // Supported input/output formats
    }
  }
}

// Model loading
message LoadModelRequest {
  string model_id = 1;
  map<string, string> options = 2;  // Loading options (quantization, etc.)
}

message LoadModelResponse {
  string model_id = 1;
  oneof response {
    LoadProgress progress = 2;
    LoadComplete complete = 3;
    gate.common.v1.Error error = 4;
  }
  
  message LoadProgress {
    string stage = 1;        // "downloading", "loading", "initializing"
    float progress_percent = 2;
    string message = 3;
    uint64 bytes_loaded = 4; // Optional: bytes loaded so far
    uint64 total_bytes = 5;  // Optional: total bytes to load
  }
  
  message LoadComplete {
    bool success = 1;
    ListModelsResponse.ModelInfo model_info = 2;
  }
}

// Model unloading
message UnloadModelRequest {
  string model_id = 1;
  bool force = 2;  // Force unload even if in use
}

message UnloadModelResponse {
  bool success = 1;
  string message = 2;  // Optional message about unload result
}

// Model status
message ModelStatusRequest {
  string model_id = 1;
}

message ModelStatusResponse {
  oneof result {
    ModelStatus status = 1;
    gate.common.v1.Error error = 2;
  }
  
  message ModelStatus {
    string model_id = 1;
    ListModelsResponse.ModelInfo.ModelStatus status = 2;
    uint32 active_requests = 3;
    uint64 memory_usage = 4;
    int64 last_activity = 5;  // Timestamp of last inference
  }
}