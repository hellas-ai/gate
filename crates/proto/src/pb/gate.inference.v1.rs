// This file is @generated by prost-build.
/// Inference request
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct InferenceRequest {
    /// Client-provided request ID for correlation
    #[prost(string, tag = "1")]
    pub request_id: ::prost::alloc::string::String,
    /// Model to use for inference
    #[prost(string, tag = "2")]
    pub model_id: ::prost::alloc::string::String,
    /// Model-specific input
    #[prost(message, optional, tag = "3")]
    pub input_data: ::core::option::Option<super::super::common::v1::JsonValue>,
    #[prost(message, optional, tag = "4")]
    pub options: ::core::option::Option<inference_request::InferenceOptions>,
}
/// Nested message and enum types in `InferenceRequest`.
pub mod inference_request {
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct InferenceOptions {
        /// Enable streaming responses
        #[prost(bool, tag = "1")]
        pub streaming: bool,
        /// Maximum tokens to generate
        #[prost(uint32, tag = "2")]
        pub max_tokens: u32,
        /// Randomness (0.0 to 2.0)
        #[prost(float, tag = "3")]
        pub temperature: f32,
        /// Stop generation on these sequences
        #[prost(string, repeated, tag = "4")]
        pub stop_sequences: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// Model-specific parameters
        #[prost(map = "string, string", tag = "5")]
        pub custom_params: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            ::prost::alloc::string::String,
        >,
    }
}
/// Inference response (used for both streaming and unary)
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct InferenceResponse {
    #[prost(string, tag = "1")]
    pub request_id: ::prost::alloc::string::String,
    #[prost(oneof = "inference_response::Response", tags = "2, 3, 4")]
    pub response: ::core::option::Option<inference_response::Response>,
}
/// Nested message and enum types in `InferenceResponse`.
pub mod inference_response {
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct InferenceChunk {
        /// Incremental data
        #[prost(message, optional, tag = "1")]
        pub data: ::core::option::Option<super::super::super::common::v1::JsonValue>,
        /// Last chunk in stream
        #[prost(bool, tag = "2")]
        pub is_final: bool,
        /// Optional metrics
        #[prost(message, optional, tag = "3")]
        pub metrics: ::core::option::Option<InferenceMetrics>,
    }
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct InferenceComplete {
        /// Final result
        #[prost(message, optional, tag = "1")]
        pub result: ::core::option::Option<super::super::super::common::v1::JsonValue>,
        #[prost(message, optional, tag = "2")]
        pub metrics: ::core::option::Option<InferenceMetrics>,
    }
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct InferenceMetrics {
        #[prost(uint32, tag = "1")]
        pub tokens_processed: u32,
        #[prost(uint32, tag = "2")]
        pub tokens_generated: u32,
        #[prost(uint64, tag = "3")]
        pub latency_ms: u64,
        #[prost(float, tag = "4")]
        pub tokens_per_second: f32,
        #[prost(map = "string, string", tag = "5")]
        pub custom_metrics: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            ::prost::alloc::string::String,
        >,
    }
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Response {
        #[prost(message, tag = "2")]
        Chunk(InferenceChunk),
        #[prost(message, tag = "3")]
        Complete(InferenceComplete),
        #[prost(message, tag = "4")]
        Error(super::super::super::common::v1::Error),
    }
}
/// Model listing
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ListModelsRequest {
    /// Include models that aren't loaded
    #[prost(bool, tag = "1")]
    pub include_unavailable: bool,
}
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsResponse {
    #[prost(message, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<list_models_response::ModelInfo>,
}
/// Nested message and enum types in `ListModelsResponse`.
pub mod list_models_response {
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ModelInfo {
        #[prost(string, tag = "1")]
        pub id: ::prost::alloc::string::String,
        #[prost(string, tag = "2")]
        pub name: ::prost::alloc::string::String,
        #[prost(string, tag = "3")]
        pub description: ::prost::alloc::string::String,
        /// "text", "code", "chat", "completion"
        #[prost(string, repeated, tag = "4")]
        pub capabilities: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        #[prost(enumeration = "model_info::ModelStatus", tag = "5")]
        pub status: i32,
        #[prost(message, optional, tag = "6")]
        pub specs: ::core::option::Option<model_info::ModelSpecs>,
    }
    /// Nested message and enum types in `ModelInfo`.
    pub mod model_info {
        #[derive(serde::Serialize, serde::Deserialize)]
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct ModelSpecs {
            /// Number of parameters
            #[prost(uint64, tag = "1")]
            pub parameters: u64,
            /// Memory required in bytes
            #[prost(uint64, tag = "2")]
            pub memory_required: u64,
            /// Supported input/output formats
            #[prost(string, repeated, tag = "3")]
            pub formats: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        }
        #[derive(serde::Serialize, serde::Deserialize)]
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum ModelStatus {
            /// Ready for inference
            Available = 0,
            /// Currently loading
            Loading = 1,
            /// Loaded but not ready (warming up)
            Loaded = 2,
            /// Failed to load
            Error = 3,
            /// Not currently loaded
            Unloaded = 4,
        }
        impl ModelStatus {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Available => "AVAILABLE",
                    Self::Loading => "LOADING",
                    Self::Loaded => "LOADED",
                    Self::Error => "ERROR",
                    Self::Unloaded => "UNLOADED",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "AVAILABLE" => Some(Self::Available),
                    "LOADING" => Some(Self::Loading),
                    "LOADED" => Some(Self::Loaded),
                    "ERROR" => Some(Self::Error),
                    "UNLOADED" => Some(Self::Unloaded),
                    _ => None,
                }
            }
        }
    }
}
/// Model loading
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LoadModelRequest {
    #[prost(string, tag = "1")]
    pub model_id: ::prost::alloc::string::String,
    /// Loading options (quantization, etc.)
    #[prost(map = "string, string", tag = "2")]
    pub options: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LoadModelResponse {
    #[prost(string, tag = "1")]
    pub model_id: ::prost::alloc::string::String,
    #[prost(oneof = "load_model_response::Response", tags = "2, 3, 4")]
    pub response: ::core::option::Option<load_model_response::Response>,
}
/// Nested message and enum types in `LoadModelResponse`.
pub mod load_model_response {
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct LoadProgress {
        /// "downloading", "loading", "initializing"
        #[prost(string, tag = "1")]
        pub stage: ::prost::alloc::string::String,
        #[prost(float, tag = "2")]
        pub progress_percent: f32,
        #[prost(string, tag = "3")]
        pub message: ::prost::alloc::string::String,
        /// Optional: bytes loaded so far
        #[prost(uint64, tag = "4")]
        pub bytes_loaded: u64,
        /// Optional: total bytes to load
        #[prost(uint64, tag = "5")]
        pub total_bytes: u64,
    }
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct LoadComplete {
        #[prost(bool, tag = "1")]
        pub success: bool,
        #[prost(message, optional, tag = "2")]
        pub model_info: ::core::option::Option<super::list_models_response::ModelInfo>,
    }
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Response {
        #[prost(message, tag = "2")]
        Progress(LoadProgress),
        #[prost(message, tag = "3")]
        Complete(LoadComplete),
        #[prost(message, tag = "4")]
        Error(super::super::super::common::v1::Error),
    }
}
/// Model unloading
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UnloadModelRequest {
    #[prost(string, tag = "1")]
    pub model_id: ::prost::alloc::string::String,
    /// Force unload even if in use
    #[prost(bool, tag = "2")]
    pub force: bool,
}
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UnloadModelResponse {
    #[prost(bool, tag = "1")]
    pub success: bool,
    /// Optional message about unload result
    #[prost(string, tag = "2")]
    pub message: ::prost::alloc::string::String,
}
/// Model status
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelStatusRequest {
    #[prost(string, tag = "1")]
    pub model_id: ::prost::alloc::string::String,
}
#[derive(serde::Serialize, serde::Deserialize)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelStatusResponse {
    #[prost(oneof = "model_status_response::Result", tags = "1, 2")]
    pub result: ::core::option::Option<model_status_response::Result>,
}
/// Nested message and enum types in `ModelStatusResponse`.
pub mod model_status_response {
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ModelStatus {
        #[prost(string, tag = "1")]
        pub model_id: ::prost::alloc::string::String,
        #[prost(
            enumeration = "super::list_models_response::model_info::ModelStatus",
            tag = "2"
        )]
        pub status: i32,
        #[prost(uint32, tag = "3")]
        pub active_requests: u32,
        #[prost(uint64, tag = "4")]
        pub memory_usage: u64,
        /// Timestamp of last inference
        #[prost(int64, tag = "5")]
        pub last_activity: i64,
    }
    #[derive(serde::Serialize, serde::Deserialize)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Result {
        #[prost(message, tag = "1")]
        Status(ModelStatus),
        #[prost(message, tag = "2")]
        Error(super::super::super::common::v1::Error),
    }
}
/// Generated client implementations.
pub mod inference_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// Inference protocol for AI model interaction
    /// Provided by daemon nodes that support inference
    #[derive(Debug, Clone)]
    pub struct InferenceServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl<T> InferenceServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::Body>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> InferenceServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::Body>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::Body>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::Body>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            InferenceServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Streaming inference for real-time token generation
        pub async fn streaming_inference(
            &mut self,
            request: impl tonic::IntoStreamingRequest<Message = super::InferenceRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::InferenceResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/gate.inference.v1.InferenceService/StreamingInference",
            );
            let mut req = request.into_streaming_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "gate.inference.v1.InferenceService",
                        "StreamingInference",
                    ),
                );
            self.inner.streaming(req, path, codec).await
        }
        /// Traditional unary inference
        pub async fn inference(
            &mut self,
            request: impl tonic::IntoRequest<super::InferenceRequest>,
        ) -> std::result::Result<
            tonic::Response<super::InferenceResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/gate.inference.v1.InferenceService/Inference",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("gate.inference.v1.InferenceService", "Inference"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Model management
        pub async fn list_models(
            &mut self,
            request: impl tonic::IntoRequest<super::ListModelsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListModelsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/gate.inference.v1.InferenceService/ListModels",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("gate.inference.v1.InferenceService", "ListModels"),
                );
            self.inner.unary(req, path, codec).await
        }
        pub async fn load_model(
            &mut self,
            request: impl tonic::IntoRequest<super::LoadModelRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::LoadModelResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/gate.inference.v1.InferenceService/LoadModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("gate.inference.v1.InferenceService", "LoadModel"),
                );
            self.inner.server_streaming(req, path, codec).await
        }
        pub async fn unload_model(
            &mut self,
            request: impl tonic::IntoRequest<super::UnloadModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::UnloadModelResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/gate.inference.v1.InferenceService/UnloadModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("gate.inference.v1.InferenceService", "UnloadModel"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Model status and health
        pub async fn get_model_status(
            &mut self,
            request: impl tonic::IntoRequest<super::ModelStatusRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ModelStatusResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/gate.inference.v1.InferenceService/GetModelStatus",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "gate.inference.v1.InferenceService",
                        "GetModelStatus",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Generated server implementations.
pub mod inference_service_server {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    /// Generated trait containing gRPC methods that should be implemented for use with InferenceServiceServer.
    #[async_trait]
    pub trait InferenceService: std::marker::Send + std::marker::Sync + 'static {
        /// Server streaming response type for the StreamingInference method.
        type StreamingInferenceStream: tonic::codegen::tokio_stream::Stream<
                Item = std::result::Result<super::InferenceResponse, tonic::Status>,
            >
            + std::marker::Send
            + 'static;
        /// Streaming inference for real-time token generation
        async fn streaming_inference(
            &self,
            request: tonic::Request<tonic::Streaming<super::InferenceRequest>>,
        ) -> std::result::Result<
            tonic::Response<Self::StreamingInferenceStream>,
            tonic::Status,
        >;
        /// Traditional unary inference
        async fn inference(
            &self,
            request: tonic::Request<super::InferenceRequest>,
        ) -> std::result::Result<
            tonic::Response<super::InferenceResponse>,
            tonic::Status,
        >;
        /// Model management
        async fn list_models(
            &self,
            request: tonic::Request<super::ListModelsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListModelsResponse>,
            tonic::Status,
        >;
        /// Server streaming response type for the LoadModel method.
        type LoadModelStream: tonic::codegen::tokio_stream::Stream<
                Item = std::result::Result<super::LoadModelResponse, tonic::Status>,
            >
            + std::marker::Send
            + 'static;
        async fn load_model(
            &self,
            request: tonic::Request<super::LoadModelRequest>,
        ) -> std::result::Result<tonic::Response<Self::LoadModelStream>, tonic::Status>;
        async fn unload_model(
            &self,
            request: tonic::Request<super::UnloadModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::UnloadModelResponse>,
            tonic::Status,
        >;
        /// Model status and health
        async fn get_model_status(
            &self,
            request: tonic::Request<super::ModelStatusRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ModelStatusResponse>,
            tonic::Status,
        >;
    }
    /// Inference protocol for AI model interaction
    /// Provided by daemon nodes that support inference
    #[derive(Debug)]
    pub struct InferenceServiceServer<T> {
        inner: Arc<T>,
        accept_compression_encodings: EnabledCompressionEncodings,
        send_compression_encodings: EnabledCompressionEncodings,
        max_decoding_message_size: Option<usize>,
        max_encoding_message_size: Option<usize>,
    }
    impl<T> InferenceServiceServer<T> {
        pub fn new(inner: T) -> Self {
            Self::from_arc(Arc::new(inner))
        }
        pub fn from_arc(inner: Arc<T>) -> Self {
            Self {
                inner,
                accept_compression_encodings: Default::default(),
                send_compression_encodings: Default::default(),
                max_decoding_message_size: None,
                max_encoding_message_size: None,
            }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> InterceptedService<Self, F>
        where
            F: tonic::service::Interceptor,
        {
            InterceptedService::new(Self::new(inner), interceptor)
        }
        /// Enable decompressing requests with the given encoding.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.accept_compression_encodings.enable(encoding);
            self
        }
        /// Compress responses with the given encoding, if the client supports it.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.send_compression_encodings.enable(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.max_decoding_message_size = Some(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.max_encoding_message_size = Some(limit);
            self
        }
    }
    impl<T, B> tonic::codegen::Service<http::Request<B>> for InferenceServiceServer<T>
    where
        T: InferenceService,
        B: Body + std::marker::Send + 'static,
        B::Error: Into<StdError> + std::marker::Send + 'static,
    {
        type Response = http::Response<tonic::body::Body>;
        type Error = std::convert::Infallible;
        type Future = BoxFuture<Self::Response, Self::Error>;
        fn poll_ready(
            &mut self,
            _cx: &mut Context<'_>,
        ) -> Poll<std::result::Result<(), Self::Error>> {
            Poll::Ready(Ok(()))
        }
        fn call(&mut self, req: http::Request<B>) -> Self::Future {
            match req.uri().path() {
                "/gate.inference.v1.InferenceService/StreamingInference" => {
                    #[allow(non_camel_case_types)]
                    struct StreamingInferenceSvc<T: InferenceService>(pub Arc<T>);
                    impl<
                        T: InferenceService,
                    > tonic::server::StreamingService<super::InferenceRequest>
                    for StreamingInferenceSvc<T> {
                        type Response = super::InferenceResponse;
                        type ResponseStream = T::StreamingInferenceStream;
                        type Future = BoxFuture<
                            tonic::Response<Self::ResponseStream>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<
                                tonic::Streaming<super::InferenceRequest>,
                            >,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as InferenceService>::streaming_inference(
                                        &inner,
                                        request,
                                    )
                                    .await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = StreamingInferenceSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.streaming(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/gate.inference.v1.InferenceService/Inference" => {
                    #[allow(non_camel_case_types)]
                    struct InferenceSvc<T: InferenceService>(pub Arc<T>);
                    impl<
                        T: InferenceService,
                    > tonic::server::UnaryService<super::InferenceRequest>
                    for InferenceSvc<T> {
                        type Response = super::InferenceResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::InferenceRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as InferenceService>::inference(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = InferenceSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/gate.inference.v1.InferenceService/ListModels" => {
                    #[allow(non_camel_case_types)]
                    struct ListModelsSvc<T: InferenceService>(pub Arc<T>);
                    impl<
                        T: InferenceService,
                    > tonic::server::UnaryService<super::ListModelsRequest>
                    for ListModelsSvc<T> {
                        type Response = super::ListModelsResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::ListModelsRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as InferenceService>::list_models(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = ListModelsSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/gate.inference.v1.InferenceService/LoadModel" => {
                    #[allow(non_camel_case_types)]
                    struct LoadModelSvc<T: InferenceService>(pub Arc<T>);
                    impl<
                        T: InferenceService,
                    > tonic::server::ServerStreamingService<super::LoadModelRequest>
                    for LoadModelSvc<T> {
                        type Response = super::LoadModelResponse;
                        type ResponseStream = T::LoadModelStream;
                        type Future = BoxFuture<
                            tonic::Response<Self::ResponseStream>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::LoadModelRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as InferenceService>::load_model(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = LoadModelSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.server_streaming(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/gate.inference.v1.InferenceService/UnloadModel" => {
                    #[allow(non_camel_case_types)]
                    struct UnloadModelSvc<T: InferenceService>(pub Arc<T>);
                    impl<
                        T: InferenceService,
                    > tonic::server::UnaryService<super::UnloadModelRequest>
                    for UnloadModelSvc<T> {
                        type Response = super::UnloadModelResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::UnloadModelRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as InferenceService>::unload_model(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = UnloadModelSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                "/gate.inference.v1.InferenceService/GetModelStatus" => {
                    #[allow(non_camel_case_types)]
                    struct GetModelStatusSvc<T: InferenceService>(pub Arc<T>);
                    impl<
                        T: InferenceService,
                    > tonic::server::UnaryService<super::ModelStatusRequest>
                    for GetModelStatusSvc<T> {
                        type Response = super::ModelStatusResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<super::ModelStatusRequest>,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as InferenceService>::get_model_status(&inner, request)
                                    .await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = GetModelStatusSvc(inner);
                        let codec = tonic::codec::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                _ => {
                    Box::pin(async move {
                        let mut response = http::Response::new(
                            tonic::body::Body::default(),
                        );
                        let headers = response.headers_mut();
                        headers
                            .insert(
                                tonic::Status::GRPC_STATUS,
                                (tonic::Code::Unimplemented as i32).into(),
                            );
                        headers
                            .insert(
                                http::header::CONTENT_TYPE,
                                tonic::metadata::GRPC_CONTENT_TYPE,
                            );
                        Ok(response)
                    })
                }
            }
        }
    }
    impl<T> Clone for InferenceServiceServer<T> {
        fn clone(&self) -> Self {
            let inner = self.inner.clone();
            Self {
                inner,
                accept_compression_encodings: self.accept_compression_encodings,
                send_compression_encodings: self.send_compression_encodings,
                max_decoding_message_size: self.max_decoding_message_size,
                max_encoding_message_size: self.max_encoding_message_size,
            }
        }
    }
    /// Generated gRPC service name
    pub const SERVICE_NAME: &str = "gate.inference.v1.InferenceService";
    impl<T> tonic::server::NamedService for InferenceServiceServer<T> {
        const NAME: &'static str = SERVICE_NAME;
    }
}
